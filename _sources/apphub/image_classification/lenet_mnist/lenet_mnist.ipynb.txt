{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification Using LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to walk through the logic in `lenet_mnist.py` shown below and provide step-by-step instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lenet_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare training and evaluation dataset, create FastEstimator `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` can take both data in memory and data in disk. In this example, we are going to use data in memory by loading data with `tf.keras.datasets.mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
    "print(\"train image shape is {}\".format(x_train.shape))\n",
    "print(\"train label shape is {}\".format(y_train.shape))\n",
    "print(\"eval image shape is {}\".format(x_eval.shape))\n",
    "print(\"eval label shape is {}\".format(y_eval.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution layer requires channel dimension (batch, height, width, channel), therefore, we need to expand the training image and evaluation image by one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_eval = np.expand_dims(x_eval, -1)\n",
    "print(\"train image shape is {}\".format(x_train.shape))\n",
    "print(\"eval image shape is {}\".format(x_eval.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For in-memory data in `Pipeline`, the data format should be a nested dictionary like: {\"mode1\": {\"feature1\": numpy_array, \"feature2\": numpy_array, ...}, ...}. Each `mode` can be either `train` or `eval`, in our case, we have both `train` and `eval`.  `feature` is the feature name, in our case, we have `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"train\": {\"x\": x_train, \"y\": y_train}, \"eval\": {\"x\": x_eval, \"y\": y_eval}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define `Pipeline`, we want to apply a `Minmax` online preprocessing to the image feature `x` for both training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.pipeline.processing import Minmax\n",
    "pipeline = fe.Pipeline(batch_size=32, data=data, ops=Minmax(inputs=\"x\", outputs=\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare model, create FastEstimator `Network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to define the network architecture in `tf.keras.Model` or `tf.keras.Sequential`, for a popular architecture like LeNet, FastEstimator has it implemented already in [fastestimator.architecture.lenet](https://github.com/fastestimator/fastestimator/blob/master/fastestimator/architecture/lenet.py).  After defining the architecture, users are expected to feed the architecture definition and its associated model name, optimizer and loss name (default to be 'loss') to `FEModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture import LeNet\n",
    "from fastestimator.network.model import FEModel\n",
    "model = FEModel(model_def=LeNet, model_name=\"lenet\", optimizer=\"adam\", loss_name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the `Network`: given with a batch data with key `x` and `y`, we have to work our way to `loss` with series of operators.  `ModelOp` is an operator that contains a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.network.model import ModelOp\n",
    "from fastestimator.network.loss import SparseCategoricalCrossentropy\n",
    "network = fe.Network(ops=[ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"), \n",
    "                          SparseCategoricalCrossentropy(y_pred=\"y_pred\", y_true=\"y\", outputs=\"loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Configure training, create `Estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training loop, we want to: 1) measure accuracy for data data 2) save the model with lowest valdiation loss. `Trace` class is used for anything related to training loop, we will need to import the `Accuracy` and `ModelSaver` trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from fastestimator.estimator.trace import Accuracy, ModelSaver\n",
    "save_dir = tempfile.mkdtemp()\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name='acc'),\n",
    "          ModelSaver(model_name=\"lenet\", save_dir=save_dir, save_best=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the `Estimator` and specify the training configuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=2, traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training\n",
    "training time takes ~1min on CPU of Mac Book Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model is saved to a temporary folder. we can load the model from file and do inferencing on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_path = os.path.join(save_dir, 'lenet_best_loss.h5')\n",
    "trained_model = tf.keras.models.load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly get one image from validation set and compare the ground truth with model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image idx 387, ground truth: 2\n",
      "model predicted class is 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_idx = np.random.randint(10000)\n",
    "print(\"test image idx {}, ground truth: {}\".format(selected_idx, y_eval[selected_idx]))\n",
    "plt.imshow(x_eval[selected_idx, :, :, 0])\n",
    "\n",
    "test_image = x_eval[selected_idx]\n",
    "test_image = np.expand_dims(test_image, 0)\n",
    "prediction_score = trained_model.predict(test_image)\n",
    "print(\"model predicted class is {}\".format(np.argmax(prediction_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
